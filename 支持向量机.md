---
created: 2025/11/20 09:51
updated: 2025/11/20 10:33
---
# 支持向量机（SVM）核心原理

支持向量机（Support Vector Machine, SVM）是基于统计学习理论的二分类模型，其核心思想是找到能最大化两类样本分类间隔的最优分离超平面；对于非线性问题，可通过核函数将样本映射到高维特征空间，转化为线性可分问题求解。

## 一、核心思想

- 目标是寻找 “最优分离超平面”，使两类样本到超平面的 “间隔” 最大；
- 超平面仅由少数 “支持向量”（离超平面最近的样本）决定，具有较强鲁棒性；
- 线性不可分问题通过核函数映射到高维特征空间，转化为线性可分问题。

## 二、线性可分 SVM 的数学模型

### 1. 超平面方程

线性超平面的数学表达式为：\(\mathbf{w} \cdot \mathbf{x} + b = 0\)其中，\(\mathbf{w}\) 为超平面的法向量（维度与样本特征一致），b 为偏置项，\(\mathbf{x}\) 为样本特征向量。

### 2. 分类决策规则

对样本 \(\mathbf{x}\)，通过符号函数判断类别：\(f(\mathbf{x}) = \text{sign}(\mathbf{w} \cdot \mathbf{x} + b)\)其中 \(\text{sign}(\cdot)\) 为符号函数：若结果为正则预测为正类（\(+1\)），为负则预测为负类（\(-1\)）。

### 3. 间隔与优化目标

- 样本到超平面的几何间隔定义为：\(\gamma_i = \frac{y_i(\mathbf{w} \cdot \mathbf{x}_i + b)}{\|\mathbf{w}\|}\)，其中 \(y_i \in \{+1, -1\}\) 为样本真实标签，\(\|\mathbf{w}\|\) 为 \(\mathbf{w}\) 的 L2 范数。
- 最大化最小几何间隔等价于最小化 \(\|\mathbf{w}\|^2\)，优化问题为：
    
    \(\min_{\mathbf{w}, b} \quad \frac{1}{2}\|\mathbf{w}\|^2\)
    
    \(\text{s.t.} \quad y_i(\mathbf{w} \cdot \mathbf{x}_i + b) \geq 1 \quad (i=1,2,\dots,n)\)
    
    其中约束条件确保所有样本都在间隔边界外侧（或边界上）。

## 三、对偶问题与核函数

### 1. 对偶优化（拉格朗日乘数法转化）

引入拉格朗日乘数 \(\alpha_i \geq 0\)，原问题可转化为对偶问题：\(\max_{\alpha} \quad \sum_{i=1}^n \alpha_i - \frac{1}{2}\sum_{i=1}^n \sum_{j=1}^n \alpha_i \alpha_j y_i y_j (\mathbf{x}_i \cdot \mathbf{x}_j)\)\(\text{s.t.} \quad \sum_{i=1}^n \alpha_i y_i = 0\)\(\alpha_i \geq 0 \quad (i=1,2,\dots,n)\)最优解 \(\alpha^*\) 中，仅支持向量对应的 \(\alpha_i^* > 0\)（非支持向量的 \(\alpha_i^* = 0\)）。

### 2. 核函数（处理非线性问题）

当样本线性不可分时，用核函数 \(K(\mathbf{x}_i, \mathbf{x}_j) = \phi(\mathbf{x}_i) \cdot \phi(\mathbf{x}_j)\) 替代内积 \(\mathbf{x}_i \cdot \mathbf{x}_j\)，其中 \(\phi(\cdot)\) 为向高维空间的映射。常见核函数包括：

- 线性核：\(K(\mathbf{x}_i, \mathbf{x}_j) = \mathbf{x}_i \cdot \mathbf{x}_j\)
- 多项式核：\(K(\mathbf{x}_i, \mathbf{x}_j) = (\mathbf{x}_i \cdot \mathbf{x}_j + c)^d\)（\(c \geq 0\) 为偏移量，d 为次数）
- 高斯核（RBF）：\(K(\mathbf{x}_i, \mathbf{x}_j) = \exp\left(-\gamma\|\mathbf{x}_i - \mathbf{x}_j\|^2\right)\)（\(\gamma > 0\) 为带宽参数）

### 3. 非线性 SVM 决策函数

引入核函数后，决策函数为：\(f(\mathbf{x}) = \text{sign}\left( \sum_{i=1}^n \alpha_i^* y_i K(\mathbf{x}, \mathbf{x}_i) + b^* \right)\)其中 \(b^*\) 由支持向量满足 \(y_i(\mathbf{w} \cdot \mathbf{x}_i + b) = 1\) 求解得到。

## 四、软间隔 SVM（应对噪声与重叠样本）

允许部分样本违反间隔约束，引入松弛变量 \(\xi_i \geq 0\)（表示样本偏离间隔边界的程度），优化目标为：\(\min_{\mathbf{w}, b, \xi} \quad \frac{1}{2}\|\mathbf{w}\|^2 + C\sum_{i=1}^n \xi_i\)\(\text{s.t.} \quad y_i(\mathbf{w} \cdot \mathbf{x}_i + b) \geq 1 - \xi_i\)\(\xi_i \geq 0 \quad (i=1,2,\dots,n)\)其中 \(C > 0\) 为惩罚系数：C 越大，对违规样本的惩罚越重（模型越倾向于严格分类）；C 越小，对噪声的容忍度越高。