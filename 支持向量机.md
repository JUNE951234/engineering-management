---
created: 2025/11/20 09:51
updated: 2025/11/20 10:59
aliases:
  - SVM
  - Support Vector Machine
---
# 支持向量机（SVM）核心原理

支持向量机（Support Vector Machine, SVM）是基于统计学习理论的二分类模型，是一种监督学习算法，用于分类和回归任务。其基本思想是找到一个最优的超平面（hyperplane），使得不同类别之间的间隔（margin）最大化。对于非线性问题，可通过核函数将样本映射到高维特征空间，转化为线性可分问题求解。


## 一、核心思想

- 目标是寻找 “最优分离超平面”，使两类样本到超平面的 “间隔” 最大；
- 超平面仅由少数 “支持向量”（离超平面最近的样本）决定，具有较强鲁棒性；
- 线性不可分问题通过核函数映射到高维特征空间，转化为线性可分问题。



## 支持向量机（SVM）简明推导

### 1 线性可分 SVM
训练集  
$$
\{(x_i,\,y_i)\}_{i=1}^n,\quad x_i\in\mathbb{R}^d,\; y_i\in\{-1,+1\}
$$

寻找超平面  
$$
w^T x + b = 0
$$

硬间隔约束  
$$
y_i(w^T x_i + b) \geq 1,\quad \forall i
$$

最大化间隔 $\Leftrightarrow$ 最小化  
$$
\min_{w,b}\; \frac{1}{2}\|w\|^2
$$

### 2 对偶问题
拉格朗日  
$$
L(w,b,\alpha)=\frac{1}{2}\|w\|^2 - \sum_{i=1}^n \alpha_i\Big[y_i(w^T x_i+b)-1\Big],\quad \alpha_i\geq 0
$$

求梯度并令为零  
$$
w = \sum_{i=1}^n \alpha_i y_i x_i,\qquad \sum_{i=1}^n \alpha_i y_i = 0
$$

代入得对偶  
$$
\max_{\alpha}\; \sum_{i=1}^n \alpha_i - \frac{1}{2}\sum_{i,j=1}^n \alpha_i\alpha_j y_i y_j x_i^T x_j
$$

约束  
$$
\sum_{i=1}^n \alpha_i y_i = 0,\quad \alpha_i\geq 0
$$

### 3 核技巧（非线性）
用核函数 $K(x_i,x_j)$ 替换内积  
$$
\max_{\alpha}\; \sum_{i=1}^n \alpha_i - \frac{1}{2}\sum_{i,j=1}^n \alpha_i\alpha_j y_i y_j K(x_i,x_j)
$$

常用核  
- 线性：$K(x_i,x_j)=x_i^T x_j$  
- 多项式：$K(x_i,x_j)=(x_i^T x_j+c)^d$  
- RBF：$K(x_i,x_j)=\exp\!\bigl(-\gamma\|x_i-x_j\|^2\bigr)$

### 4 软间隔
引入松弛变量 $\xi_i\geq 0$ 与惩罚参数 $C>0$  
$$
\min_{w,b,\xi}\; \frac{1}{2}\|w\|^2 + C\sum_{i=1}^n \xi_i
$$

s.t.  
$$
y_i(w^T x_i+b)\geq 1-\xi_i,\quad \xi_i\geq 0,\quad \forall i
$$

